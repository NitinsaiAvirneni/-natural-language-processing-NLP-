{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import roman\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: roman in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\nitin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install openpyxl\n",
    "!pip install nltk\n",
    "!pip install roman\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nitin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('input.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each row and extract URL and URL_ID\n",
    "urls = []\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    urls.append((url_id, url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_title = soup.find('title').get_text()\n",
    "    article_text = ''\n",
    "    article_body = soup.find('body')\n",
    "\n",
    "    for p in article_body.find_all('p'):\n",
    "        if p.parent.name not in ['header', 'footer', 'nav']:\n",
    "            article_text += p.get_text() + '\\n'\n",
    "\n",
    "    return article_title, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('extracted_articles', exist_ok=True)\n",
    "\n",
    "for url_id, url in urls:\n",
    "    article_title, article_text = extract_article_text(url)\n",
    "    \n",
    "    file_name = re.sub(r'[^\\w\\s]', '', url_id) + '.txt'\n",
    "    file_path = os.path.join('extracted_articles', file_name)\n",
    "\n",
    "    with open(file_path, 'w',encoding='utf-8') as f:\n",
    "        f.write(article_text[1099:-82])\n",
    "\n",
    "    # print(f'Article extracted and saved: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words from .txt files in a folder\n",
    "def load_stop_words(folder_path):\n",
    "    stop_words = set()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith('.txt'):  # Check if it's a text file\n",
    "            with open(file_path, 'r', encoding='latin1') as file:\n",
    "                for line in file:\n",
    "                    stop_words.add(line.strip()) \n",
    "    return stop_words\n",
    "\n",
    "# for roman words \n",
    "def is_roman_numeral(word):\n",
    "    try:\n",
    "        roman.fromRoman(word)\n",
    "        return True\n",
    "    except roman.InvalidRomanNumeralError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "#To load text from .txt files in a folder\n",
    "text_data_dict = {}\n",
    "def text_data_extracted_articles(folder_path):\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text_data = file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, 'r', encoding='latin1') as file:\n",
    "                    text_data = file.read()\n",
    "            \n",
    "            # Replace Roman numerals with their equivalent integers\n",
    "            words = text_data.split()\n",
    "            for i in range(len(words)):\n",
    "                if is_roman_numeral(words[i]):\n",
    "                    words[i] = str(roman.fromRoman(words[i]))\n",
    "            \n",
    "                text_data = ' '.join(words)\n",
    "                text_data_dict[file_name] = text_data   \n",
    "    return text_data_dict\n",
    "    \n",
    "text_data_dict = text_data_extracted_articles('extracted_articles')\n",
    "\n",
    "\n",
    "# to clean up text with stop words single texts\n",
    "def clean_text_with_custom_stopwords(text, custom_stop_words):\n",
    "    words = word_tokenize(text)\n",
    "   \n",
    "    filtered_words = [word for word in words if word.lower() not in custom_stop_words]\n",
    "   \n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "    print(cleaned_text)\n",
    "\n",
    " \n",
    "# to clean up text with stop words multi texts\n",
    "def clean_and_save_text(text, stop_words, output_folder, file_name):\n",
    "    cleaned_text = clean_text_with_custom_stopwords(text,stop_words)\n",
    "    output_path = os.path.join(output_folder, f\"{file_name}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved as 'blackassign0001.txt.txt'\n",
      "Cleaned text saved as 'blackassign0002.txt.txt'\n",
      "Cleaned text saved as 'blackassign0003.txt.txt'\n",
      "Cleaned text saved as 'blackassign0004.txt.txt'\n",
      "Cleaned text saved as 'blackassign0005.txt.txt'\n",
      "Cleaned text saved as 'blackassign0006.txt.txt'\n",
      "Cleaned text saved as 'blackassign0007.txt.txt'\n",
      "Cleaned text saved as 'blackassign0008.txt.txt'\n",
      "Cleaned text saved as 'blackassign0009.txt.txt'\n",
      "Cleaned text saved as 'blackassign0010.txt.txt'\n",
      "Cleaned text saved as 'blackassign0011.txt.txt'\n",
      "Cleaned text saved as 'blackassign0012.txt.txt'\n",
      "Cleaned text saved as 'blackassign0013.txt.txt'\n",
      "Cleaned text saved as 'blackassign0014.txt.txt'\n",
      "Cleaned text saved as 'blackassign0015.txt.txt'\n",
      "Cleaned text saved as 'blackassign0016.txt.txt'\n",
      "Cleaned text saved as 'blackassign0017.txt.txt'\n",
      "Cleaned text saved as 'blackassign0018.txt.txt'\n",
      "Cleaned text saved as 'blackassign0019.txt.txt'\n",
      "Cleaned text saved as 'blackassign0020.txt.txt'\n",
      "Cleaned text saved as 'blackassign0021.txt.txt'\n",
      "Cleaned text saved as 'blackassign0022.txt.txt'\n",
      "Cleaned text saved as 'blackassign0023.txt.txt'\n",
      "Cleaned text saved as 'blackassign0024.txt.txt'\n",
      "Cleaned text saved as 'blackassign0025.txt.txt'\n",
      "Cleaned text saved as 'blackassign0026.txt.txt'\n",
      "Cleaned text saved as 'blackassign0027.txt.txt'\n",
      "Cleaned text saved as 'blackassign0028.txt.txt'\n",
      "Cleaned text saved as 'blackassign0029.txt.txt'\n",
      "Cleaned text saved as 'blackassign0030.txt.txt'\n",
      "Cleaned text saved as 'blackassign0031.txt.txt'\n",
      "Cleaned text saved as 'blackassign0032.txt.txt'\n",
      "Cleaned text saved as 'blackassign0033.txt.txt'\n",
      "Cleaned text saved as 'blackassign0034.txt.txt'\n",
      "Cleaned text saved as 'blackassign0035.txt.txt'\n",
      "Cleaned text saved as 'blackassign0036.txt.txt'\n",
      "Cleaned text saved as 'blackassign0037.txt.txt'\n",
      "Cleaned text saved as 'blackassign0038.txt.txt'\n",
      "Cleaned text saved as 'blackassign0039.txt.txt'\n",
      "Cleaned text saved as 'blackassign0040.txt.txt'\n",
      "Cleaned text saved as 'blackassign0041.txt.txt'\n",
      "Cleaned text saved as 'blackassign0042.txt.txt'\n",
      "Cleaned text saved as 'blackassign0043.txt.txt'\n",
      "Cleaned text saved as 'blackassign0044.txt.txt'\n",
      "Cleaned text saved as 'blackassign0045.txt.txt'\n",
      "Cleaned text saved as 'blackassign0046.txt.txt'\n",
      "Cleaned text saved as 'blackassign0047.txt.txt'\n",
      "Cleaned text saved as 'blackassign0048.txt.txt'\n",
      "Cleaned text saved as 'blackassign0049.txt.txt'\n",
      "Cleaned text saved as 'blackassign0050.txt.txt'\n",
      "Cleaned text saved as 'blackassign0051.txt.txt'\n",
      "Cleaned text saved as 'blackassign0052.txt.txt'\n",
      "Cleaned text saved as 'blackassign0053.txt.txt'\n",
      "Cleaned text saved as 'blackassign0054.txt.txt'\n",
      "Cleaned text saved as 'blackassign0055.txt.txt'\n",
      "Cleaned text saved as 'blackassign0056.txt.txt'\n",
      "Cleaned text saved as 'blackassign0057.txt.txt'\n",
      "Cleaned text saved as 'blackassign0058.txt.txt'\n",
      "Cleaned text saved as 'blackassign0059.txt.txt'\n",
      "Cleaned text saved as 'blackassign0060.txt.txt'\n",
      "Cleaned text saved as 'blackassign0061.txt.txt'\n",
      "Cleaned text saved as 'blackassign0062.txt.txt'\n",
      "Cleaned text saved as 'blackassign0063.txt.txt'\n",
      "Cleaned text saved as 'blackassign0064.txt.txt'\n",
      "Cleaned text saved as 'blackassign0065.txt.txt'\n",
      "Cleaned text saved as 'blackassign0066.txt.txt'\n",
      "Cleaned text saved as 'blackassign0067.txt.txt'\n",
      "Cleaned text saved as 'blackassign0068.txt.txt'\n",
      "Cleaned text saved as 'blackassign0069.txt.txt'\n",
      "Cleaned text saved as 'blackassign0070.txt.txt'\n",
      "Cleaned text saved as 'blackassign0071.txt.txt'\n",
      "Cleaned text saved as 'blackassign0072.txt.txt'\n",
      "Cleaned text saved as 'blackassign0073.txt.txt'\n",
      "Cleaned text saved as 'blackassign0074.txt.txt'\n",
      "Cleaned text saved as 'blackassign0075.txt.txt'\n",
      "Cleaned text saved as 'blackassign0076.txt.txt'\n",
      "Cleaned text saved as 'blackassign0077.txt.txt'\n",
      "Cleaned text saved as 'blackassign0078.txt.txt'\n",
      "Cleaned text saved as 'blackassign0079.txt.txt'\n",
      "Cleaned text saved as 'blackassign0080.txt.txt'\n",
      "Cleaned text saved as 'blackassign0081.txt.txt'\n",
      "Cleaned text saved as 'blackassign0082.txt.txt'\n",
      "Cleaned text saved as 'blackassign0083.txt.txt'\n",
      "Cleaned text saved as 'blackassign0084.txt.txt'\n",
      "Cleaned text saved as 'blackassign0085.txt.txt'\n",
      "Cleaned text saved as 'blackassign0086.txt.txt'\n",
      "Cleaned text saved as 'blackassign0087.txt.txt'\n",
      "Cleaned text saved as 'blackassign0088.txt.txt'\n",
      "Cleaned text saved as 'blackassign0089.txt.txt'\n",
      "Cleaned text saved as 'blackassign0090.txt.txt'\n",
      "Cleaned text saved as 'blackassign0091.txt.txt'\n",
      "Cleaned text saved as 'blackassign0092.txt.txt'\n",
      "Cleaned text saved as 'blackassign0093.txt.txt'\n",
      "Cleaned text saved as 'blackassign0094.txt.txt'\n",
      "Cleaned text saved as 'blackassign0095.txt.txt'\n",
      "Cleaned text saved as 'blackassign0096.txt.txt'\n",
      "Cleaned text saved as 'blackassign0097.txt.txt'\n",
      "Cleaned text saved as 'blackassign0098.txt.txt'\n",
      "Cleaned text saved as 'blackassign0099.txt.txt'\n",
      "Cleaned text saved as 'blackassign0100.txt.txt'\n"
     ]
    }
   ],
   "source": [
    "stop_words = load_stop_words(\"StopWords\")\n",
    "\n",
    "folder_path=\"extracted_articles\"\n",
    "\n",
    "text_data_extracted_articles(folder_path)\n",
    "\n",
    "output_folder = 'cleaned_texts'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file_name, text in text_data_dict.items():\n",
    "    clean_and_save_text(text, stop_words, output_folder, file_name)\n",
    "    print(f\"Cleaned text saved as '{file_name}.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################use to cross the stops and the content#############\n",
    "# def read_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#             content = file.read()                   \n",
    "#     except UnicodeDecodeError:\n",
    "#                 with open(file_path, 'r', encoding='latin1') as file:\n",
    "#                     content = file.read()\n",
    "#     return content\n",
    "\n",
    "# def get_matching_words(file1_content, file2_content):\n",
    "#     words_file1 = set(file1_content.split())\n",
    "#     words_file2 = set(file2_content.split())\n",
    "#     matching_words = words_file1.intersection(words_file2)\n",
    "#     return matching_words\n",
    "\n",
    "# file1_path = 'text.txt'\n",
    "# file2_path = 'stop.txt'\n",
    "\n",
    "# file1_content = read_file(file1_path)\n",
    "# file2_content = read_file(file2_path)\n",
    "\n",
    "# matching_words = get_matching_words(file1_content, file2_content)\n",
    "\n",
    "# if matching_words:\n",
    "#     print(\"The files have matching words:\")\n",
    "#     print(matching_words)\n",
    "# else:\n",
    "#     print(\"The files do not have any matching words.\")\n",
    "\n",
    "#####################use to cross the stops and the content#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_positive_negative_stopwords(folder_path):\n",
    "    \n",
    "    positive_stopwords = set()\n",
    "    \n",
    "    negative_stopwords = set()\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith('.txt'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for word in file:\n",
    "                        word = word.strip()\n",
    "                        if 'positive' in file_name:\n",
    "                            positive_stopwords.add(word)\n",
    "                        elif 'negative' in file_name:\n",
    "                            negative_stopwords.add(word)\n",
    "                        \n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, 'r', encoding='latin1') as file:\n",
    "                    for word in file:\n",
    "                        word = word.strip()\n",
    "                        if 'positive' in file_name:\n",
    "                            positive_stopwords.add(word)\n",
    "                        elif 'negative' in file_name:\n",
    "                            negative_stopwords.add(word)\n",
    "                            \n",
    "    return positive_stopwords, negative_stopwords\n",
    "\n",
    "positive_stopwords, negative_stopwords = load_positive_negative_stopwords(\"MasterDictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned texts\n",
    "def load_cleaned_texts(folder_path):\n",
    "    cleaned_texts = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_name.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                cleaned_texts[file_name] = text\n",
    "    return cleaned_texts\n",
    "\n",
    "# Count positive and negative words in cleaned texts\n",
    "def count_positive_negative_words(cleaned_texts, positive_stopwords, negative_stopwords):\n",
    "    positive_counts = {}\n",
    "    negative_counts = {}\n",
    "    for file_name, text in cleaned_texts.items():\n",
    "        positive_count = sum(1 for word in text.split() if word.lower() in positive_stopwords)\n",
    "        negative_count = sum(1 for word in text.split() if word.lower() in negative_stopwords)\n",
    "        positive_counts[file_name] = positive_count\n",
    "        negative_counts[file_name] = negative_count\n",
    "    return positive_counts, negative_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_stopwords, negative_stopwords = load_positive_negative_stopwords(\"MasterDictionary\")\n",
    "cleaned_texts = load_cleaned_texts(\"cleaned_texts\")\n",
    "positive_counts, negative_counts = count_positive_negative_words(cleaned_texts, positive_stopwords, negative_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Positive Word Counts:\")\n",
    "# print(positive_counts)\n",
    "# print(\"\\nNegative Word Counts:\")\n",
    "# print(negative_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuations_and_save_text(text_data,output_folder, file_name):\n",
    "                    sentences = sent_tokenize(text_data)\n",
    "                    words = word_tokenize(text_data)\n",
    "                    cleaned_words = [word.lower() for word in words if word not in string.punctuation]\n",
    "                    word_string = ' '.join(cleaned_words)\n",
    "                    output_path = os.path.join(output_folder, f\"{file_name}\")\n",
    "                    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                        output_file.write(word_string)\n",
    "                        \n",
    "\n",
    "text_data_punctuations_dict=text_data_extracted_articles('cleaned_texts')\n",
    "\n",
    "output_folder = 'cleaned_punctuations_texts'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "for file_name ,text in text_data_punctuations_dict.items():\n",
    "    clean_punctuations_and_save_text(text,output_folder, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_Score_counts(cleaned_words,text,Positive,Negative):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    positive_score = (Positive)\n",
    "    \n",
    "    negative_score = (Negative)\n",
    "    \n",
    "    polarity_score = ((positive_score) - (negative_score)) / ((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "    \n",
    "    avg_sentence_length = len(cleaned_words) / len(sentences)\n",
    "    \n",
    "    complex_words = [word for word in cleaned_words if len(word) > 2]  \n",
    "    \n",
    "    percentage_complex_words = len(complex_words) / len(cleaned_words)\n",
    "    \n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    avg_words_per_sentence = len(cleaned_words) / len(sentences)\n",
    "    \n",
    "    complex_word_count = len([word for word in cleaned_words if len(word) > 2])\n",
    "    \n",
    "    cleaned_text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    words = cleaned_text.split()\n",
    "    word_count=len(words)\n",
    "    \n",
    "    def syllable_count(word):\n",
    "        word = word.lower()\n",
    "        if word.endswith(('es', 'ed')):\n",
    "            return 0  \n",
    "        vowels = 'aeiou'\n",
    "        count = 0\n",
    "        for index, char in enumerate(word):\n",
    "            if char in vowels and (index == 0 or word[index - 1] not in vowels):\n",
    "                count += 1\n",
    "        return max(count, 1) \n",
    "    syllable_count_per_word = [syllable_count(word) for word in cleaned_words]\n",
    "\n",
    "    personal_pronouns = re.findall(r'\\b(?:i|we|my|ours|us)\\b', text.lower())\n",
    "    personal_pronoun_count = len(personal_pronouns)\n",
    "    \n",
    "    average_word_length = sum(len(word) for word in cleaned_words) / len(cleaned_words)\n",
    "    \n",
    "    return positive_score,negative_score,polarity_score,subjectivity_score,avg_sentence_length,percentage_complex_words,fog_index,avg_words_per_sentence,complex_word_count,syllable_count_per_word,personal_pronoun_count,average_word_length,word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_read_data_punctuations_dict=text_data_extracted_articles('cleaned_punctuations_texts')\n",
    "positive_stopwords, negative_stopwords = load_positive_negative_stopwords(\"MasterDictionary\")\n",
    "def all_values():\n",
    "    for file_name ,text in text_read_data_punctuations_dict.items():\n",
    "        \n",
    "        word_list = text.split()\n",
    "        \n",
    "        if positive_counts.get(file_name)==None:\n",
    "            Positive=0\n",
    "        else:\n",
    "            Positive=positive_counts.get(file_name)\n",
    "        if negative_counts.get(file_name)==None:\n",
    "            Negative=0\n",
    "        else:\n",
    "            Negative=negative_counts.get(file_name)\n",
    "        print(file_name)\n",
    "        positive_score,negative_score,polarity_score,subjectivity_score,avg_sentence_length,percentage_complex_words,fog_index,avg_words_per_sentence,complex_word_count,syllable_count_per_word,personal_pronoun_count,average_word_length,word_count=All_Score_counts(word_list,text,Positive,Negative)\n",
    "    # print(file_name)    \n",
    "    # return positive_score,negative_score,polarity_score,subjectivity_score,avg_sentence_length,percentage_complex_words,fog_index,avg_words_per_sentence,complex_word_count,syllable_count_per_word,personal_pronoun_count,average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackassign0001\n",
      "blackassign0002\n",
      "blackassign0003\n",
      "blackassign0004\n",
      "blackassign0005\n",
      "blackassign0006\n",
      "blackassign0007\n",
      "blackassign0008\n",
      "blackassign0009\n",
      "blackassign0010\n",
      "blackassign0011\n",
      "blackassign0012\n",
      "blackassign0013\n",
      "blackassign0014\n",
      "blackassign0015\n",
      "blackassign0016\n",
      "blackassign0017\n",
      "blackassign0018\n",
      "blackassign0019\n",
      "blackassign0020\n",
      "blackassign0021\n",
      "blackassign0022\n",
      "blackassign0023\n",
      "blackassign0024\n",
      "blackassign0025\n",
      "blackassign0026\n",
      "blackassign0027\n",
      "blackassign0028\n",
      "blackassign0029\n",
      "blackassign0030\n",
      "blackassign0031\n",
      "blackassign0032\n",
      "blackassign0033\n",
      "blackassign0034\n",
      "blackassign0035\n",
      "blackassign0036\n",
      "blackassign0037\n",
      "blackassign0038\n",
      "blackassign0039\n",
      "blackassign0040\n",
      "blackassign0041\n",
      "blackassign0042\n",
      "blackassign0043\n",
      "blackassign0044\n",
      "blackassign0045\n",
      "blackassign0046\n",
      "blackassign0047\n",
      "blackassign0048\n",
      "blackassign0049\n",
      "blackassign0050\n",
      "blackassign0051\n",
      "blackassign0052\n",
      "blackassign0053\n",
      "blackassign0054\n",
      "blackassign0055\n",
      "blackassign0056\n",
      "blackassign0057\n",
      "blackassign0058\n",
      "blackassign0059\n",
      "blackassign0060\n",
      "blackassign0061\n",
      "blackassign0062\n",
      "blackassign0063\n",
      "blackassign0064\n",
      "blackassign0065\n",
      "blackassign0066\n",
      "blackassign0067\n",
      "blackassign0068\n",
      "blackassign0069\n",
      "blackassign0070\n",
      "blackassign0071\n",
      "blackassign0072\n",
      "blackassign0073\n",
      "blackassign0074\n",
      "blackassign0075\n",
      "blackassign0076\n",
      "blackassign0077\n",
      "blackassign0078\n",
      "blackassign0079\n",
      "blackassign0080\n",
      "blackassign0081\n",
      "blackassign0082\n",
      "blackassign0083\n",
      "blackassign0084\n",
      "blackassign0085\n",
      "blackassign0086\n",
      "blackassign0087\n",
      "blackassign0088\n",
      "blackassign0089\n",
      "blackassign0090\n",
      "blackassign0091\n",
      "blackassign0092\n",
      "blackassign0093\n",
      "blackassign0094\n",
      "blackassign0095\n",
      "blackassign0096\n",
      "blackassign0097\n",
      "blackassign0098\n",
      "blackassign0099\n",
      "blackassign0100\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Load the Excel workbook\n",
    "def outputcsv():\n",
    "    workbook = openpyxl.load_workbook('Output Data Structure.xlsx')\n",
    "    sheet = workbook.active\n",
    "    for file_name ,text in text_read_data_punctuations_dict.items():\n",
    "        word_list = text.split()\n",
    "        if positive_counts.get(file_name)==None:\n",
    "            Positive=0\n",
    "        else:\n",
    "            Positive=positive_counts.get(file_name)\n",
    "            \n",
    "        if negative_counts.get(file_name)==None:\n",
    "            Negative=0\n",
    "        else:\n",
    "            Negative=negative_counts.get(file_name)\n",
    "        positive_score,negative_score,polarity_score,subjectivity_score,avg_sentence_length,percentage_complex_words,fog_index,avg_words_per_sentence,complex_word_count,syllable_count_per_word,personal_pronoun_count,average_word_length,word_count=All_Score_counts(word_list,text,Positive,Negative)\n",
    "        remove=\".txt\"\n",
    "        name=file_name.replace(remove,'')\n",
    "        for i in range(2,102):\n",
    "            if sheet['A'+str(i)].value == name :\n",
    "                sheet['C'+str(i)] = positive_score #POSITIVE SCORE\n",
    "                sheet['D'+str(i)] = negative_score  #NEGATIVE SCORE\n",
    "                \n",
    "                sheet['E'+str(i)] = polarity_score  #POLARITY SCORE\n",
    "                \n",
    "                sheet['F'+str(i)] = subjectivity_score #SUBJECTIVITY SCORE\n",
    "\n",
    "                sheet['G'+str(i)] = avg_sentence_length  #AVG SENTENCE LENGTH\n",
    "\n",
    "                sheet['H'+str(i)] = percentage_complex_words  #PERCENTAGE OF COMPLEX WORDS\n",
    "\n",
    "                sheet['I'+str(i)] = fog_index #FOG INDEX\n",
    "\n",
    "                sheet['J'+str(i)] = avg_words_per_sentence #AVG NUMBER OF WORDS PER SENTENCE\n",
    "\n",
    "                sheet['K'+str(i)] = complex_word_count  #COMPLEX WORD COUNT\n",
    "\n",
    "                sheet['L'+str(i)] = word_count  # WORD COUNT\n",
    "\n",
    "                sheet['M'+str(i)] = \"-\"  #SYLLABLE PER WORD beacause of list\n",
    "\n",
    "                sheet['N'+str(i)] = personal_pronoun_count #PERSONAL PRONOUNS\n",
    "\n",
    "                sheet['O'+str(i)] = average_word_length  #AVG WORD LENGTH\n",
    "        print(name)\n",
    "\n",
    "    workbook.save('Final Output.xlsx')\n",
    "    \n",
    "outputcsv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
